---
layout: post
title:  "Notes about PCA"
date:   2018-02-28 18:02:35 +0300
categories: ml
---
>Underlined item like $$\underline{U_1}$$ means vector, uppercase letter like $$X$$ means matrix, lowercase like $$x$$ means scalar variable.

The notes here is a summary of what I have learned in some lectures online and the related articles.
Let's begin with the problem, we have a matrix
\begin{equation} 
X = \begin{bmatrix} \underline{x_1} & \underline{x_2} &\dots & \underline{x_n}
\end{bmatrix}_{dxn}
\end{equation}

where $$\underline{x_i}\epsilon\Re^d$$
> Buraya boyutlarlar ilgili resmi koy

We would like to find some kind of new coordinate system based on $$U_1$$ , $$U_2$$ and aim to choose the direction with maximum variance to represent better the original data.
So mathematically speaking we are going to solve this incomplete optimization problem.
\begin{array}{ll}
\underset{\underline{U_1}}{\text{maximize}}& \text{var} (\underline{U_1}^TX)\\\
\end{array}

where  $$\underline{U_1}\epsilon\Re^{1xd}$$ and $$X\epsilon\Re^{dxn}$$ so resulting reduced vector along the dimension of $$U_1$$ is $$\Re^{1xn}$$
Let's think about the case where we want to multiply the value of mean and variance. If the multiplier is $$z$$ then mean will increase $$z$$ times, but for the variance the amount of increase will be $$z^2$$. Based on the behavior of variance with a multiplier, we can deduce that:

\begin{equation}
var(\underline{U_1}X) = \underline{U^T}S\underline{U_1}
\end{equation}
where $$S$$ is the covariance matrix of $$X$$ and the result of the operation expected to be scalar.
What we have here is an optimisation problem for a quadratic function with an expected scalar result (you can verify that from the dimensions of components). To solve that optimisation problem we need more constraints since without further limitations we can just choose maximum values for $$\underline{U_1}$$.  An appropriate constraint could be $$\underline{U_1}^T \underline{U_1} = 1$$. With this addition to maximizatiion problem, finally we have well defined problem.
\begin{array}{ll}
\underset{\underline{U_1}}{\text{maximize}}& \text{var} (\underline{U_1}^TX)\\\
\text{subject to} & \underline{U^T} \underline{U_1} = 1\\\
\end{array}

The obvious method to solve such an optimization problem is to use Lagragian. Quick note about the optimization, we aim to maximize a function based on another function at some point where their derivative are aligned.
> buraya su f ve g nin seklini koy
> sonra lagrange hesaplamasini koy


